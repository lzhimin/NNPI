{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The main purpose of this experiment is to quantify the difference of difference feature of neural network after a certain amount of network pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "# import the model of neural network\n",
    "from python.model import LeNet, LeNet_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "parser = argparse.ArgumentParser(\n",
    "    description='PyTorch MNIST pruning from deep compression paper')\n",
    "parser.add_argument('--batch-size', type=int, default=100, metavar='N',\n",
    "                    help='input batch size for training (default: 50)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                    help='input batch size for testing (default: 1000)')\n",
    "parser.add_argument('--epochs', type=int, default=100, metavar='N',\n",
    "                    help='number of epochs to train (default: 100)')\n",
    "parser.add_argument('--lr', type=float, default=0.1, metavar='LR',\n",
    "                    help='learning rate (default: 0.01)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='disables CUDA training')\n",
    "parser.add_argument('--seed', type=int, default=42, metavar='S',\n",
    "                    help='random seed (default: 42)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "parser.add_argument('--log', type=str, default='log.txt',\n",
    "                    help='log file name')\n",
    "parser.add_argument('--sensitivity', type=float, default=2,\n",
    "                    help=\"sensitivity value that is multiplied to layer's std in order to get threshold value\")\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "#the device used for training\n",
    "device = 'cuda'#torch.device(\"cuda\" if use_cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the training data\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor()\n",
    "                   ])),\n",
    "    batch_size=args.batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "# load the testing data\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor()\n",
    "                   ])),\n",
    "    batch_size=args.test_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, model, device, optimizer):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "\n",
    "        for batch_idx, (data, target) in pbar:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        #save(model, str(epoch))\n",
    "        test(model, device)\n",
    "\n",
    "\n",
    "def test(model, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    flag = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            # sum up batch loss\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            # get the index of the max log-probability\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        accuracy = 100. * correct / len(test_loader.dataset)\n",
    "        print('Test set: Average loss:', test_loss,\n",
    "              'Accuracy', correct/len(test_loader.dataset))\n",
    "\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inner_State_Difference_Quantification:\n",
    "    def __init__(self, model, pruned_model, samples):\n",
    "        self.model = model\n",
    "        self.pruned_model = model\n",
    "        self.samples = samples\n",
    "        \n",
    "    def getInnerStateRepresentation(self):\n",
    "        return self.model.getInnerState(self.samples)\n",
    "    \n",
    "    def feature_map(self):\n",
    "        pass\n",
    "    \n",
    "    def knn(self):\n",
    "        pass\n",
    "\n",
    "    def pca(self):\n",
    "        pass\n",
    "\n",
    "    def gradient_of_neuron(self):\n",
    "        pass\n",
    "\n",
    "    def weight(self):\n",
    "        pass\n",
    "\n",
    "    def saliencyMap(self):\n",
    "        pass\n",
    "\n",
    "    def featureVis(self):\n",
    "        pass\n",
    "\n",
    "    def critical_neuron_activation_ranking(self):\n",
    "        pass\n",
    "\n",
    "    #\n",
    "    def input_vs_output(self):\n",
    "        pass\n",
    "\n",
    "    ###such as?\n",
    "    def others(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load different neural network model\n",
    "model = LeNet(mask=True).to(device)\n",
    "#model = LeNet_5(mask=True).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('../data/model/LetNet/letnet300_trained.pkl'))\n",
    "#model.prune_by_percentile(99)\n",
    "#test(model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
