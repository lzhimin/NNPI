{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Quantify the difference of difference feature of neural network after a certain amount of network pruning.\n",
    "### 2. Understand the subnetwork overlap between different neural network samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "# import the model of neural network\n",
    "from python.model import LeNet, LeNet_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "parser = argparse.ArgumentParser(\n",
    "    description='PyTorch MNIST pruning from deep compression paper')\n",
    "parser.add_argument('--batch-size', type=int, default=50, metavar='N',\n",
    "                    help='input batch size for training (default: 50)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                    help='input batch size for testing (default: 1000)')\n",
    "parser.add_argument('--epochs', type=int, default=100, metavar='N',\n",
    "                    help='number of epochs to train (default: 100)')\n",
    "parser.add_argument('--lr', type=float, default=0.1, metavar='LR',\n",
    "                    help='learning rate (default: 0.01)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='disables CUDA training')\n",
    "parser.add_argument('--seed', type=int, default=42, metavar='S',\n",
    "                    help='random seed (default: 42)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "parser.add_argument('--log', type=str, default='log.txt',\n",
    "                    help='log file name')\n",
    "parser.add_argument('--sensitivity', type=float, default=2,\n",
    "                    help=\"sensitivity value that is multiplied to layer's std in order to get threshold value\")\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "#the device used for training\n",
    "device = 'cpu'#torch.device(\"cuda\" if use_cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    }
   ],
   "source": [
    "# load the training data\n",
    "mnist = datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor()\n",
    "                   ]))\n",
    "train_loader = torch.utils.data.DataLoader(mnist,batch_size=args.batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "# load the testing data\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor()\n",
    "                   ])),\n",
    "    batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, model, device, optimizer):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "\n",
    "        for batch_idx, (data, target) in pbar:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        #save(model, str(epoch))\n",
    "        test(model, device)\n",
    "\n",
    "\n",
    "def test(model, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    flag = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            # sum up batch loss\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            # get the index of the max log-probability\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        accuracy = 100. * correct / len(test_loader.dataset)\n",
    "        print('Test set: Average loss:', test_loss,\n",
    "              'Accuracy', correct/len(test_loader.dataset))\n",
    "\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inner_State_Difference_Quantification:\n",
    "    def __init__(self, model, pruned_model, samples):\n",
    "        self.model = model\n",
    "        self.pruned_model = model\n",
    "        self.samples = samples\n",
    "        \n",
    "    def getInnerStateRepresentation(self):\n",
    "        return self.model.getInnerState(self.samples)\n",
    "    \n",
    "    def feature_map(self):\n",
    "        pass\n",
    "\n",
    "    def pca(self):\n",
    "        pass\n",
    "\n",
    "    def gradient_of_neuron(self):\n",
    "        pass\n",
    "\n",
    "    def weight(self):\n",
    "        pass\n",
    "\n",
    "    def saliencyMap(self):\n",
    "        pass\n",
    "\n",
    "    def critical_neuron_activation_ranking(self):\n",
    "        pass\n",
    "\n",
    "    #\n",
    "    def input_vs_output(self):\n",
    "        pass\n",
    "\n",
    "    ###such as?\n",
    "    def others(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.058854640824027936 Accuracy 0.9814\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "98.14"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load different neural network model\n",
    "#model = LeNet(mask=True).to(device)\n",
    "model = LeNet_5(mask=True).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=0.0001)\n",
    "#model.load_state_dict(torch.load('../data/model/LetNet/model_9.pkl'))\n",
    "model.load_state_dict(torch.load('../data/model/LetNet/letnet_5_trained.pkl'))\n",
    "\n",
    "#model.prune_by_percentile(95)\n",
    "#train(10, model, device, optimizer)\n",
    "test(model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# subnetwork overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#path = os.path.join('../data/model/LetNet', 'model_{}.pkl'.format('odd'))\n",
    "#torch.save(model.state_dict(), path)\n",
    "dataset = {}\n",
    "count = 0\n",
    "\n",
    "for data, target in test_loader:\n",
    "    device_data, device_target = data.to('cpu'), target.to('cpu')\n",
    "    label = device_target.item()\n",
    "    \n",
    "    if label in dataset:\n",
    "        dataset[label].append(device_data.tolist())\n",
    "    else:\n",
    "        if label == 0 and count > 0:\n",
    "            count-=1\n",
    "            continue\n",
    "        dataset[label] = [device_data.tolist()]\n",
    "    \n",
    "keys = list(dataset.keys())\n",
    "keys.sort()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1 = {}\n",
    "fc2 = {}\n",
    "layer1 = {}\n",
    "layer2 = {}\n",
    "for k in keys:\n",
    "    rs = model.activationPattern(dataset[k])\n",
    "    fc1[k] = rs['fc1']\n",
    "    fc2[k] = rs['fc2']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in keys:\n",
    "    layer1[k] = set(np.where(np.array(fc1[k]) > 10)[0].tolist())\n",
    "    layer2[k] = set(np.where(np.array(fc2[k]) > 10)[0].tolist())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "287 97\n",
      "layer1 0.9860627177700348\n",
      "layer2 1.0\n",
      "layer1 1.0\n",
      "layer2 1.0\n",
      "layer1 1.0\n",
      "layer2 1.0\n",
      "layer1 0.9965156794425087\n",
      "layer2 0.9896907216494846\n",
      "layer1 0.9965156794425087\n",
      "layer2 0.9896907216494846\n",
      "layer1 1.0\n",
      "layer2 1.0\n",
      "layer1 0.9965156794425087\n",
      "layer2 1.0\n",
      "layer1 1.0\n",
      "layer2 1.0\n",
      "layer1 1.0\n",
      "layer2 1.0\n",
      "layer1 1.0\n",
      "layer2 1.0\n"
     ]
    }
   ],
   "source": [
    "index = 1\n",
    "print(len(layer1[index]), len(layer2[index]))\n",
    "for i in range(0, 10):\n",
    "    print('layer1',len(layer1[index] & layer1[i])/len(layer1[index]))\n",
    "    print('layer2',len(layer2[index] & layer2[i])/len(layer2[index]))\n",
    "# &  layer1[2] & layer1[3] & layer1[4] & layer1[5] & layer1[6] & layer1[7] & layer1[8] & layer1[9]) \n",
    "#print(layer1[0] | layer1[1] |  layer1[2] | layer1[3] | layer1[4] | layer1[5] | layer1[6] | layer1[7] | layer1[8] | layer1[9]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(layer2[0] & layer2[1] &  layer2[2] & layer2[3] & layer2[4] & layer2[5] & layer2[6] & layer2[7] & layer2[8] & layer2[9]) \n",
    "#print(layer2[0] | layer2[1] |  layer2[2] | layer2[3] | layer2[4] | layer2[5] | layer2[6] | layer2[7] | layer2[8] | layer2[9]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "980"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
